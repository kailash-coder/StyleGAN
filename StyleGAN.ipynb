{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Q_kq_TkNKbW"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\asus\\anaconda3\\lib\\site-packages (2.1.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\asus\\anaconda3\\lib\\site-packages (0.16.1)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\asus\\anaconda3\\lib\\site-packages (2.1.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torch) (2023.4.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torchvision) (1.24.3)\n",
      "Requirement already satisfied: requests in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from torchvision) (9.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests->torchvision) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests->torchvision) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: torchsummary in c:\\users\\asus\\anaconda3\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: pyprind in c:\\users\\asus\\anaconda3\\lib\\site-packages (2.11.3)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\asus\\anaconda3\\lib\\site-packages (4.8.1.78)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from opencv-python) (1.24.3)\n",
      "Requirement already satisfied: google in c:\\users\\asus\\anaconda3\\lib\\site-packages (3.0.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from google) (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from beautifulsoup4->google) (2.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\asus\\anaconda3\\lib\\site-packages (3.7.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from matplotlib) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from matplotlib) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from matplotlib) (9.4.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision torchaudio\n",
    "!pip3 install torchsummary\n",
    "!pip3 install pyprind\n",
    "!pip3 install opencv-python\n",
    "!pip3 install google\n",
    "!pip3 install matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "ep-oAiHS4s0l"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mglob\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpatches\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cv2_imshow\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgridspec\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgridspec\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os, glob\n",
    "\n",
    "import cv2\n",
    "from google.colab.patches import cv2_imshow\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "o7D4vZWvM4Xt"
   },
   "outputs": [],
   "source": [
    "class CreateDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, PATH, image_size=4):\n",
    "        self.PATH = PATH\n",
    "        self.image_size = image_size\n",
    "        self.transform = self._get_transform_(self.image_size)\n",
    "        self.entry = glob.glob(os.path.join(self.PATH, \"*.jpg\"))\n",
    "\n",
    "    def _get_transform_(self, image_size):\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize(image_size),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "        return transform\n",
    "\n",
    "    def grow(self):\n",
    "        self.image_size *= 2\n",
    "        self.transform = self._get_transform_(self.image_size)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = cv2.imread(self.entry[index], cv2.IMREAD_COLOR)\n",
    "        image = image.transpose((2, 0, 1))/255\n",
    "        image = torch.from_numpy(image).float()\n",
    "        image = self.transform(image)\n",
    "\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "blqSb6npskR_"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "lfGttot1sXIX"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ljzjEew3KDj"
   },
   "outputs": [],
   "source": [
    "class PixelNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PixelNorm, self).__init__()\n",
    "        self.eps = 1e-8\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x / (torch.mean(x**2, dim=1, keepdim=True) + self.eps) ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O7YO1AeQ4IJI"
   },
   "outputs": [],
   "source": [
    "class EqualizedConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, pad):\n",
    "        super(EqualizedConv2d, self).__init__()\n",
    "        conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, pad)\n",
    "\n",
    "        conv.weight.data.normal_()\n",
    "        conv.bias.data.zero_()\n",
    "\n",
    "        self.conv = equal_lr(conv)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z3X6xV6I4SCk"
   },
   "outputs": [],
   "source": [
    "class EqualizedLinear(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(EqualizedLinear, self).__init__()\n",
    "        linear = nn.Linear(in_channels, out_channels)\n",
    "\n",
    "        linear.weight.data.normal_()\n",
    "        linear.bias.data.zero_()\n",
    "\n",
    "        self.linear = equal_lr(linear)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zEQ_q1YK4R8m"
   },
   "outputs": [],
   "source": [
    "class AdaIn(nn.Module):\n",
    "    def __init__(self, style_dim, channel):\n",
    "        super(AdaIn, self).__init__()\n",
    "\n",
    "        self.channel = channel\n",
    "\n",
    "        self.instance_norm = nn.InstanceNorm2d(channel)\n",
    "        self.linear = EqualizedLinear(style_dim, channel * 2)\n",
    "\n",
    "    def forward(self, x, style):\n",
    "        style = self.linear(style).view(2, -1, self.channel, 1, 1)\n",
    "\n",
    "        x = self.instance_norm(x)\n",
    "        x = (x * (style[0] + 1)) + style[1]\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "89CwEoN-4R2W"
   },
   "outputs": [],
   "source": [
    "class NoiseInjection_Util(nn.Module):\n",
    "    def __init__(self, channel):\n",
    "        super(NoiseInjection_Util, self).__init__()\n",
    "\n",
    "        self.weight = nn.Parameter(torch.zeros(1, channel, 1, 1))\n",
    "\n",
    "    def forward(self, x, noise):\n",
    "        return x + self.weight * noise\n",
    "\n",
    "class NoiseInjection(nn.Module):\n",
    "    def __init__(self, channel):\n",
    "        super(NoiseInjection, self).__init__()\n",
    "\n",
    "        injection = NoiseInjection_Util(channel)\n",
    "        self.injection = equal_lr(injection)\n",
    "\n",
    "    def forward(self, x, noise):\n",
    "        return self.injection(x, noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MEEqmm7q47yg"
   },
   "outputs": [],
   "source": [
    "class EqualLR:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "\n",
    "    def compute_weight(self, module):\n",
    "        weight = getattr(module, self.name + '_orig')\n",
    "        fan_in = weight.data.size(1) * weight.data[0][0].numel()\n",
    "\n",
    "        return weight * sqrt(2 / fan_in)\n",
    "\n",
    "    @staticmethod\n",
    "    def apply(module, name):\n",
    "        fn = EqualLR(name)\n",
    "\n",
    "        weight = getattr(module, name)\n",
    "        del module._parameters[name]\n",
    "        module.register_parameter(name + '_orig', nn.Parameter(weight.data))\n",
    "        module.register_forward_pre_hook(fn)\n",
    "\n",
    "        return fn\n",
    "\n",
    "    def __call__(self, module, input):\n",
    "        weight = self.compute_weight(module)\n",
    "        setattr(module, self.name, weight)\n",
    "\n",
    "\n",
    "def equal_lr(module, name='weight'):\n",
    "    EqualLR.apply(module, name)\n",
    "\n",
    "    return module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wjEObxdi52in"
   },
   "outputs": [],
   "source": [
    "class minibatch_stddev_layer(nn.Module):\n",
    "    def __init__(self, group_size=4, num_new_features=1):\n",
    "        super(minibatch_stddev_layer, self).__init__()\n",
    "        self.group_size = group_size\n",
    "        self.num_new_features = num_new_features\n",
    "\n",
    "    def forward(self, x):\n",
    "        group_size = min(self.group_size, x.size(0))\n",
    "        origin_shape = x.shape\n",
    "        # split group\n",
    "        y = x.view(\n",
    "            group_size,\n",
    "            -1,\n",
    "            self.num_new_features,\n",
    "            origin_shape[1] // self.num_new_features,\n",
    "            origin_shape[2],\n",
    "            origin_shape[3]\n",
    "        )\n",
    "\n",
    "        # calculate stddev over group\n",
    "        y = torch.sqrt(torch.mean((y - torch.mean(y, dim=0, keepdim=True)) ** 2, dim=0) + 1e-8)\n",
    "        # [G, F. C, H, W]\n",
    "        y = torch.mean(y, dim=[2,3,4], keepdim=True)\n",
    "        # [G, F, 1, 1, 1]\n",
    "        y = torch.squeeze(y, dim=2)\n",
    "        # [G, F, 1, 1]\n",
    "        y = y.repeat(group_size, 1, origin_shape[2], origin_shape[3])\n",
    "        # [B, F, H, W]\n",
    "\n",
    "        return torch.cat([x, y], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AyynoT5f6Sui"
   },
   "outputs": [],
   "source": [
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, style_dim, prev=None):\n",
    "        super(UpBlock, self).__init__()\n",
    "\n",
    "        self.prev = prev\n",
    "\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
    "\n",
    "        if prev:\n",
    "            self.conv1 = EqualizedConv2d(in_channel, out_channel, 3, 1, 1)\n",
    "        else:\n",
    "            self.input = nn.Parameter(torch.randn(1, out_channel, 4, 4))\n",
    "\n",
    "        self.noisein1 = NoiseInjection(out_channel)\n",
    "        self.lrelu1 = nn.LeakyReLU(0.2)\n",
    "        self.adain1 = AdaIn(style_dim, out_channel)\n",
    "\n",
    "        self.conv2 = EqualizedConv2d(out_channel, out_channel, 3, 1, 1)\n",
    "        self.noisein2 = NoiseInjection(out_channel)\n",
    "        self.lrelu2 = nn.LeakyReLU(0.2)\n",
    "        self.adain2 = AdaIn(style_dim, out_channel)\n",
    "\n",
    "        self.to_rgb = EqualizedConv2d(out_channel, 3, 1, 1, 0)\n",
    "\n",
    "    # if last layer (0 <= alpha <= 1) -> return RGB image (3 channels)\n",
    "    # else return feature map of prev layer\n",
    "    def forward(self, x, style, alpha=-1.0, noise=None):\n",
    "        if self.prev: # if module has prev, then forward first.\n",
    "            w, style = style[-1], style[:-1] # pop last style\n",
    "            prev_x = x = self.prev(x, style)\n",
    "\n",
    "            x = self.upsample(x)\n",
    "\n",
    "            x = self.conv1(x)\n",
    "        else: # else initial constant\n",
    "            w = style[0]\n",
    "            x = self.input.repeat(w.size(0), 1, 1, 1)\n",
    "\n",
    "        noise = noise if noise else torch.randn(x.size(0), 1, x.size(2), x.size(3), device=x.device)\n",
    "\n",
    "        x = self.noisein1(x, noise)\n",
    "        x = self.lrelu1(x)\n",
    "        x = self.adain1(x, w)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.noisein2(x, noise)\n",
    "        x = self.lrelu2(x)\n",
    "        x = self.adain2(x, w)\n",
    "\n",
    "        if 0.0 <= alpha < 1.0:\n",
    "            prev_rgb = self.prev.to_rgb(self.upsample(prev_x))\n",
    "            x = alpha * self.to_rgb(x) + (1 - alpha) * prev_rgb\n",
    "        elif alpha == 1:\n",
    "            x = self.to_rgb(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6x6ViWUa7GvK"
   },
   "outputs": [],
   "source": [
    "class DownBlock(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, next=None):\n",
    "        super(DownBlock, self).__init__()\n",
    "\n",
    "        self.next = next\n",
    "\n",
    "        self.downsample = nn.Upsample(scale_factor=0.5, mode='bilinear', align_corners=False)\n",
    "\n",
    "        if next:\n",
    "            self.conv1 = EqualizedConv2d(in_channel, out_channel, 3, 1, 1)\n",
    "            self.conv2 = EqualizedConv2d(out_channel, out_channel, 3, 1, 1)\n",
    "        else:\n",
    "            self.conv1 = nn.Sequential(\n",
    "                minibatch_stddev_layer(),\n",
    "                EqualizedConv2d(in_channel + 1, out_channel, 3, 1, 1),\n",
    "            )\n",
    "            self.conv2 = EqualizedConv2d(out_channel, out_channel, 4, 1, 0)\n",
    "\n",
    "            self.linear = EqualizedLinear(out_channel, 1)\n",
    "\n",
    "        self.lrelu1 = nn.LeakyReLU(0.2)\n",
    "        self.lrelu2 = nn.LeakyReLU(0.2)\n",
    "\n",
    "        self.from_rgb = EqualizedConv2d(3, in_channel, 1, 1, 0)\n",
    "\n",
    "    def forward(self, x, alpha=-1.0):\n",
    "        input = x\n",
    "\n",
    "        if 0 <= alpha:\n",
    "            x = self.from_rgb(x)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.lrelu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.lrelu2(x)\n",
    "\n",
    "        if self.next:\n",
    "            x = self.downsample(x)\n",
    "\n",
    "            if 0.0 <= alpha < 1.0:\n",
    "                input = self.downsample(input)\n",
    "                x = alpha * x + (1 - alpha) * self.next.from_rgb(input)\n",
    "\n",
    "            x = self.next(x)\n",
    "        else:\n",
    "            x = x.view(x.size(0), -1)\n",
    "            x = self.linear(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rQqwJDw_tjp6"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lC9-58Al2_Av"
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, channels, style_dim, style_depth):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.style_dim = style_dim\n",
    "        self.now_growth = 1\n",
    "        self.channels = channels\n",
    "\n",
    "        self.model = UpBlock(channels[0], channels[1], style_dim, prev=None)\n",
    "\n",
    "        layers = [PixelNorm()]\n",
    "        for _ in range(style_depth):\n",
    "            layers.append(EqualizedLinear(style_dim, style_dim))\n",
    "            layers.append(nn.LeakyReLU(0.2))\n",
    "\n",
    "        self.style_mapper = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, z, alpha):\n",
    "        if type(z) not in (tuple, list):\n",
    "            w = self.style_mapper(z)\n",
    "            w = [w for _ in range(self.now_growth)]\n",
    "        else:\n",
    "            assert len(z) == 2  # now, only support mix two styles\n",
    "            w1, w2 = self.style_mapper(z[0]), self.style_mapper(z[1])\n",
    "            point = random.randint(1, self.now_growth-1)\n",
    "            # layer_0 ~ layer_p: style with w1\n",
    "            # layer_p ~ layer_n: style with w2\n",
    "            w = [w1 for _ in range(point)] + [w2 for _ in range(point, self.now_growth)]\n",
    "\n",
    "        x = self.model(x=None, style=w, alpha=alpha)\n",
    "        return x\n",
    "\n",
    "    def grow(self):\n",
    "        in_c, out_c = self.channels[self.now_growth], self.channels[self.now_growth+1]\n",
    "        self.model = UpBlock(in_c, out_c, self.style_dim, prev=self.model)\n",
    "        self.now_growth += 1\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GwEb_a0d6KhU"
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.now_growth = 1\n",
    "        self.channels = channels\n",
    "\n",
    "        self.model = DownBlock(channels[1], channels[0], next=None)\n",
    "\n",
    "    def forward(self, x, alpha):\n",
    "        return self.model(x=x, alpha=alpha)\n",
    "\n",
    "    def grow(self):\n",
    "        in_c, out_c = self.channels[self.now_growth+1], self.channels[self.now_growth]\n",
    "        self.model = DownBlock(in_c, out_c, next=self.model)\n",
    "        self.now_growth += 1\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T66NKRcU7NVu"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A5vNh5zF6Nds"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "from torch.autograd import grad\n",
    "import gc\n",
    "import time\n",
    "import pyprind\n",
    "from math import *\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5b-RXjpLSMGn"
   },
   "outputs": [],
   "source": [
    "# import contextlib\n",
    "# from apex import amp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7tojvh567YXf"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "CHECKPOINT = \"/content/Desktop/Projects/checkpoints/\"   #Here copy the path of the 'checkpoints' file \n",
    "DATA = \"/content/Desktop/Projects/dataset/CelebAMask-HQ/CelebAMask-HQ/CelebA-HQ-img/\"  #Here copy the path of the 'CelebA-HQ-img' file from the datset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1QLYsBqwKfLX"
   },
   "outputs": [],
   "source": [
    "def requires_grad(model, flag=True):\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D31iLI2WHNWx"
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, DATA, CHECKPOINT,\n",
    "                 generator_channels = [512, 512, 512, 512, 512, 256, 128, 64, 32, 16],\n",
    "                 discriminator_channels = [512, 512, 512, 512, 512, 256, 128, 64, 32, 16],\n",
    "                 style_dim = 512,\n",
    "                 style_depth = 8,\n",
    "                 lrs = {'128':0.0015, '256':0.002, '512':0.003, '1024':0.003},\n",
    "                 betas = [0.0, 0.99],\n",
    "                 batch_size = {'8':128, '16':64, '32':32, '64':16, '128':16, '256':16, '512':8, '1024':8}):\n",
    "        self.DATA = DATA\n",
    "        self.CHECKPOINT = CHECKPOINT\n",
    "        self.generator_channels = generator_channels\n",
    "        self.discriminator_channels = discriminator_channels\n",
    "        self.style_dim = style_dim\n",
    "        self.style_depth = style_depth\n",
    "        self.lrs = lrs\n",
    "        self.betas = betas\n",
    "        self.batch_size = batch_size\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.dataset = CreateDataset(PATH=self.DATA)\n",
    "        self.generator = Generator(channels=self.generator_channels, style_dim=self.style_dim, style_depth=self.style_depth).to(device)\n",
    "        self.discriminator = Discriminator(channels=self.discriminator_channels).to(device)\n",
    "\n",
    "        self.epochs = {'8':16, '16':16, '32':32, '64':64, '128':128, '256':128, '512':128, '1024':256}\n",
    "\n",
    "    def grow(self):\n",
    "        self.generator = self.generator.grow().to(device)\n",
    "        self.discriminator = self.discriminator.grow().to(device)\n",
    "        self.dataset = self.dataset.grow()\n",
    "        self.dataloader = DataLoader(self.dataset, batch_size=self.batch_size[str(self.dataset.image_size)], shuffle=True, drop_last=True)\n",
    "\n",
    "        self.lr = self.lrs.get(str(self.dataset.image_size), 0.001)\n",
    "        self.style_lr = self.lr * 0.01\n",
    "\n",
    "        self.optimizer_d = optim.Adam(params=self.discriminator.parameters(), lr=self.lr, betas=self.betas)\n",
    "        self.optimizer_g = optim.Adam([\n",
    "                {'params': self.generator.model.parameters(), 'lr':self.lr},\n",
    "                {'params': self.generator.style_mapper.parameters(), 'lr': self.style_lr},],\n",
    "            betas=self.betas)\n",
    "\n",
    "    def train_generator(self, batch_size, alpha):\n",
    "        requires_grad(self.generator, True)\n",
    "        requires_grad(self.discriminator, False)\n",
    "\n",
    "        if random.random() < 0.9:\n",
    "            z = [torch.randn(batch_size, self.style_dim).to(device),\n",
    "                 torch.randn(batch_size, self.style_dim).to(device)]\n",
    "        else:\n",
    "            z = torch.randn(batch_size, self.style_dim).to(device)\n",
    "\n",
    "        fake = self.generator(z, alpha=alpha)\n",
    "        d_fake = self.discriminator(fake, alpha=alpha)\n",
    "        loss = F.softplus(-d_fake).mean()\n",
    "        loss.backward()\n",
    "\n",
    "        self.optimizer_g.zero_grad()\n",
    "\n",
    "        self.optimizer_g.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    def train_discriminator(self, real, batch_size, alpha):\n",
    "        requires_grad(self.generator, False)\n",
    "        requires_grad(self.discriminator, True)\n",
    "\n",
    "        real.requires_grad = True\n",
    "        self.optimizer_d.zero_grad()\n",
    "\n",
    "        d_real = self.discriminator(real, alpha=alpha)\n",
    "        loss_real = F.softplus(-d_real).mean()\n",
    "        loss_real.backward(retain_graph=True)\n",
    "\n",
    "        grad_real = grad(outputs=d_real.sum(), inputs=real, create_graph=True)[0]\n",
    "        grad_penalty = (grad_real.view(grad_real.size(0), -1).norm(2, dim=1) ** 2).mean()\n",
    "        grad_penalty = 10 / 2 * grad_penalty\n",
    "        grad_penalty.backward()\n",
    "\n",
    "        if random.random() < 0.9:\n",
    "            z = [torch.randn(batch_size, self.style_dim).to(device),\n",
    "                 torch.randn(batch_size, self.style_dim).to(device)]\n",
    "        else:\n",
    "            z = torch.randn(batch_size, self.style_dim).to(device)\n",
    "\n",
    "        fake = self.generator(z, alpha=alpha)\n",
    "        d_fake = self.discriminator(fake, alpha=alpha)\n",
    "        loss_fake = F.softplus(d_fake).mean()\n",
    "        loss_fake.backward()\n",
    "\n",
    "        loss = loss_real + loss_fake + grad_penalty\n",
    "        self.optimizer_d.step()\n",
    "\n",
    "        return loss.item(), (d_real.mean().item(), d_fake.mean().item())\n",
    "\n",
    "    def run(self):\n",
    "        flag, start_epoch = self.load_checkpoint()\n",
    "        if flag:\n",
    "            self.grow()\n",
    "\n",
    "        self.generator.train()\n",
    "        self.discriminator.train()\n",
    "\n",
    "        while True:\n",
    "            for epoch in range(start_epoch+1, self.epochs[str(self.dataset.image_size)]+1):\n",
    "                print(\"Starting Epoch[{0}/{1}] | Image Size: {2}\".format(epoch, self.epochs[str(self.dataset.image_size)], self.dataset.image_size))\n",
    "                time.sleep(2)\n",
    "                trained = 0\n",
    "                epoch_loss_generator = 0\n",
    "                epoch_loss_discriminator = 0\n",
    "\n",
    "                bar = pyprind.ProgBar(len(self.dataloader), bar_char='█')\n",
    "                for idx, batch in enumerate(self.dataloader, 1):\n",
    "                    real = batch.to(device)\n",
    "                    batch_size = batch.size(0)\n",
    "                    trained += idx*batch_size\n",
    "                    alpha = min(1, trained/len(self.dataset)) if self.dataset.image_size > 8 else 1\n",
    "\n",
    "                    loss_d, (real_score, fake_score) = self.train_discriminator(real, real.size(0), alpha)\n",
    "                    loss_g = self.train_generator(real.size(0), alpha)\n",
    "\n",
    "                    epoch_loss_generator += loss_g/len(self.dataloader)\n",
    "                    epoch_loss_discriminator += loss_d/len(self.dataloader)\n",
    "\n",
    "                    bar.update()\n",
    "                    gc.collect()\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "                time.sleep(2)\n",
    "                if epoch%2==0:\n",
    "                    self.save_checkpoint(False, epoch)\n",
    "                print(\"Finished Epoch[{0}/{1}] | Image Size: {2} | Training Loss: Generator: {3} Discriminator: {4}\".format(epoch, self.epochs[str(self.dataset.image_size)], self.dataset.image_size, epoch_loss_generator, epoch_loss_discriminator))\n",
    "\n",
    "            start_epoch = 0\n",
    "            self.save_checkpoint(True)\n",
    "            self.grow()\n",
    "            if self.dataset.image_size > 1024:\n",
    "                break\n",
    "\n",
    "    def save_checkpoint(self, flag, epoch=0):\n",
    "        torch.save({\n",
    "            'generator': self.generator.state_dict(),\n",
    "            'discriminator': self.discriminator.state_dict(),\n",
    "            'generator_optimizer': self.optimizer_g.state_dict(),\n",
    "            'discriminator_optimizer': self.optimizer_d.state_dict(),\n",
    "            'image_size': self.dataset.image_size,\n",
    "            'flag': flag,\n",
    "            'epoch': epoch,\n",
    "        }, os.path.join(self.CHECKPOINT, \"model.pth\"))\n",
    "        if flag:\n",
    "            torch.save({\n",
    "                'generator': self.generator.state_dict(),\n",
    "                'discriminator': self.discriminator.state_dict(),\n",
    "                'image_size': self.dataset.image_size,\n",
    "            }, os.path.join(self.CHECKPOINT, \"model-{}x{}.pth\".format(self.dataset.image_size, self.dataset.image_size)))\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        if os.path.exists(os.path.join(self.CHECKPOINT, \"model.pth\")):\n",
    "            checkpoint = torch.load(os.path.join(self.CHECKPOINT, \"model.pth\"))\n",
    "\n",
    "            while self.dataset.image_size < checkpoint['image_size']:\n",
    "                self.grow()\n",
    "\n",
    "            self.generator.load_state_dict(checkpoint['generator'])\n",
    "            self.discriminator.load_state_dict(checkpoint['discriminator'])\n",
    "            self.optimizer_g.load_state_dict(checkpoint['generator_optimizer'])\n",
    "            self.optimizer_d.load_state_dict(checkpoint['discriminator_optimizer'])\n",
    "            flag = checkpoint.get('flag', True)\n",
    "            start_epoch = checkpoint.get('epoch', 0)\n",
    "        return flag, start_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7--16uByO-HP"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(DATA=DATA, CHECKPOINT=CHECKPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MCNHpqCgyRS0",
    "outputId": "eaab4d1d-6c4f-4064-c525-2eb35aabae04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch[1/128] | Image Size: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3658: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:57:54\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch[1/128] | Image Size: 128 | Training Loss: Generator: 30.04295074017752 Discriminator: 0.02054401841978544\n",
      "Starting Epoch[2/128] | Image Size: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:57:28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch[2/128] | Image Size: 128 | Training Loss: Generator: 26.07823207251222 Discriminator: 0.03891965297576968\n",
      "Starting Epoch[3/128] | Image Size: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:57:23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch[3/128] | Image Size: 128 | Training Loss: Generator: 21.871109827065045 Discriminator: 0.011312830857971374\n",
      "Starting Epoch[4/128] | Image Size: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:56:59\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch[4/128] | Image Size: 128 | Training Loss: Generator: 21.696052251067613 Discriminator: 0.012424071531160753\n",
      "Starting Epoch[5/128] | Image Size: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:57:08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch[5/128] | Image Size: 128 | Training Loss: Generator: 18.1580650021595 Discriminator: 0.014652086085057818\n",
      "Starting Epoch[6/128] | Image Size: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:57:08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch[6/128] | Image Size: 128 | Training Loss: Generator: 21.040044392652167 Discriminator: 0.014477847542737919\n",
      "Starting Epoch[7/128] | Image Size: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:57:19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch[7/128] | Image Size: 128 | Training Loss: Generator: 22.320557130126993 Discriminator: 0.01247927731937573\n",
      "Starting Epoch[8/128] | Image Size: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:57:19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch[8/128] | Image Size: 128 | Training Loss: Generator: 21.157467854917005 Discriminator: 0.00512684483335276\n",
      "Starting Epoch[9/128] | Image Size: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:57:18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch[9/128] | Image Size: 128 | Training Loss: Generator: 22.2637050118577 Discriminator: 0.014131062502185042\n",
      "Starting Epoch[10/128] | Image Size: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:57:23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch[10/128] | Image Size: 128 | Training Loss: Generator: 19.78081999731065 Discriminator: 0.004524140426508769\n",
      "Starting Epoch[11/128] | Image Size: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:57:30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch[11/128] | Image Size: 128 | Training Loss: Generator: 20.854679094264398 Discriminator: 0.006905785411448833\n",
      "Starting Epoch[12/128] | Image Size: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:57:30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch[12/128] | Image Size: 128 | Training Loss: Generator: 19.800023644504765 Discriminator: 0.008491408402045897\n",
      "Starting Epoch[13/128] | Image Size: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [███                           ] 100% | ETA: 00:52:08"
     ]
    }
   ],
   "source": [
    "trainer.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-QMdUKirzVj"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nQmyXP8LS6Wa"
   },
   "outputs": [],
   "source": [
    "class Inferencer:\n",
    "    def __init__(self, CHECKPOINT,\n",
    "                 generator_channels = [512, 512, 512, 512, 512, 256, 128, 64, 32, 16],\n",
    "                 style_dim = 512,\n",
    "                 style_depth = 8):\n",
    "        self.CHECKPOINT = CHECKPOINT\n",
    "        self.style_dim = style_dim\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.image_size = 4\n",
    "        self.generator = Generator(generator_channels, style_dim, style_depth).to(self.device)\n",
    "\n",
    "        self.predictions = []\n",
    "\n",
    "    def inference(self, n, image_size):\n",
    "        test_z = torch.randn(n, self.style_dim).to(self.device)\n",
    "\n",
    "        self.load_checkpoint(image_size)\n",
    "        self.generator.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            fake = self.generator(test_z, alpha=1)\n",
    "            fake = (fake + 1) * 0.5\n",
    "            fake = torch.clamp(fake, min=0.0, max=1.0)\n",
    "            fake = F.interpolate(fake, size=(256, 256))\n",
    "            fake = fake.detach().cpu().numpy()\n",
    "\n",
    "            for index in range(n):\n",
    "                self.predictions.append(np.moveaxis(fake[index], 0, -1)*255)\n",
    "\n",
    "        return self.predictions\n",
    "\n",
    "    def grow(self):\n",
    "        self.generator = self.generator.grow().to(device)\n",
    "        self.image_size *= 2\n",
    "\n",
    "    def load_checkpoint(self, image_size):\n",
    "        if os.path.exists(os.path.join(self.CHECKPOINT, \"model-{}x{}.pth\".format(image_size, image_size))):\n",
    "            checkpoint = torch.load(os.path.join(self.CHECKPOINT, \"model-{}x{}.pth\".format(image_size, image_size)))\n",
    "\n",
    "            while self.image_size < checkpoint['image_size']:\n",
    "                self.grow()\n",
    "\n",
    "            assert self.image_size == checkpoint['image_size']\n",
    "            self.generator.load_state_dict(checkpoint['generator'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9EL2wGr5W4_m"
   },
   "outputs": [],
   "source": [
    "infrencer = Inferencer(CHECKPOINT=CHECKPOINT)\n",
    "\n",
    "predictions = infrencer.inference(64, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mLW1oI-hPY0l"
   },
   "outputs": [],
   "source": [
    "_, axis = plt.subplots(len(predictions)//4, 4, figsize=(16, len(predictions)))\n",
    "axis = axis.flatten()\n",
    "for image, ax in zip(predictions, axis):\n",
    "    ax.imshow(image.astype('uint8'))\n",
    "    ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ckOwKSSdQH9-"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
